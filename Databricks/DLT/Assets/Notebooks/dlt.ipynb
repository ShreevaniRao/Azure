{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc3255d-eaa9-47cf-bb49-9b82cdc54fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b0221d-7d1d-470e-918b-213300b1f1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read from configuration settings the order status to create dynamic tables for each order status type\n",
    "order_status = spark.conf.get(\"orderStatus\", \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42147b82-2418-421c-b3dc-c5031d6a4852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality rules as dictionary for Orders & Customer data (warn (default), drop or fail) - Rule name & its value\n",
    "order_rules ={\n",
    "    \"Valid Order Status\" : \"o_orderstatus in ('F', 'O')\",\n",
    "    \"Valid Order Price\" : \"o_totalprice > 0\"\n",
    "}\n",
    "\n",
    "customer_rules ={\n",
    "    \"Valid Market Segment\": \"c_mktsegment is not null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5aa927f-e616-4e1b-879b-0c1d2482c842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming table for Orders\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"This table contains all the data from the bronze layer for Orders.\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  }\n",
    ")\n",
    "@dlt.expect_all_or_drop(order_rules) #drop\n",
    "def orders_bronze():\n",
    "  return spark.readStream.table(\"`dlt-catalog`.etl.orders_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd37525-7dc3-4ea3-b30a-2efe9d5b2097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Orders data from the landing files folder using autoloader\n",
    "@dlt.table(\n",
    "  comment=\"Incremental Order data from Autoloader\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  },\n",
    "  name = \"orders_autoloader_bronze\"\n",
    ")\n",
    "\n",
    "def load_autoloader():\n",
    "  df = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.schemaHints\", \"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"none\") \\\n",
    "    .load(\"/Volumes/dlt-catalog/etl/autoloader/files/\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97c7e08d-07cd-489f-9615-742cebcbeffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine the data from both tables using append_flow and write to a new table to be able to append streaming source and not just union the 2 datasets.\n",
    "\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "@dlt.append_flow(target = \"orders_union_bronze\")\n",
    "def order_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "\n",
    "@dlt.append_flow(target = \"orders_union_bronze\")\n",
    "def order_append_incremental():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d297db71-6852-49a3-95e1-1db87b5570e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create materialized view for Customer\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"This table contains all the data from the bronze layer for Customer.\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  }\n",
    ")\n",
    "def customer_bronze():\n",
    "  return spark.read.table(\"`dlt-catalog`.etl.customer_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe1c80d2-0660-4809-8fc0-668a17730849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming view for Customer, which will be the source for the SCD 1 & SCD 2 table. To be able to use Apply_Changes the source has to be a streaming. Commented the above code to use this streaming for the SCD tables\n",
    "\n",
    "@dlt.view(\n",
    "   comment=\"This table contains all the data from the bronze layer for Customer.\"\n",
    " )\n",
    "\n",
    "def customer_bronze_view():\n",
    "  return spark.readStream.table(\"`dlt-catalog`.etl.customer_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b0c0507c-54bb-41c4-be9c-2c3bd45847c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD 1 table - for tracking , deletes, truncates and upserts\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "dlt.create_streaming_table(\"customer_scd1_bronze\")\n",
    "dlt.apply_changes(\n",
    "  target = \"customer_scd1_bronze\",\n",
    "  source = \"customer_bronze_view\",    \n",
    "  keys = [\"c_custkey\"],\n",
    "  stored_as_scd_type = 1,\n",
    "  sequence_by = \"_dlt_timestamp\",\n",
    "  apply_as_deletes = expr(\"_dlt_change_type = 'D'\"),\n",
    "  apply_as_truncates = expr(\"_dlt_change_type = 'T'\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f630e2d-5b21-4b23-a905-9cc01e808bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SCD 2 table - for tracking history of changes\n",
    "dlt.create_streaming_table(\"customer_scd2_bronze\")\n",
    "dlt.apply_changes(\n",
    "  target = \"customer_scd2_bronze\",\n",
    "  source = \"customer_bronze_view\",    \n",
    "  keys = [\"c_custkey\"],\n",
    "  stored_as_scd_type = 2,\n",
    "  sequence_by = \"_dlt_timestamp\",\n",
    "  except_column_list = [\"_dlt_change_type\", \"_dlt_timestamp\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ab45465-7dab-4719-be32-11983b18463a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a view to join Orders and Customer (testing before SCD)\n",
    "@dlt.view(\n",
    "  comment=\"This view contains all the data from the bronze layer for Orders and Customer.\"\n",
    ")\n",
    "def order_customer_view():\n",
    "  df_customer = spark.read.table(\"LIVE.customer_bronze\")\n",
    "  df_orders = spark.read.table(\"LIVE.orders_bronze\")\n",
    "  \n",
    "  return df_orders.join(df_customer, how=\"left_outer\", on=df_orders[\"o_custkey\"] == df_customer[\"c_custkey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e6a374-7255-4451-87d6-faef26766261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view to join Orders and Customer, Comment the above code to use the SCD tables.\n",
    "\n",
    "@dlt.view(\n",
    "  comment=\"This view contains all the data from the bronze layer for Orders and Customer.\"\n",
    ")\n",
    "@dlt.expect_all_or_drop(order_rules) #drop\n",
    "@dlt.expect_all(customer_rules) #warn\n",
    "def order_customer_view():\n",
    "  df_customer = spark.read.table(\"LIVE.customer_scd2_bronze\").where (\"__END_AT IS NULL\")\n",
    "  df_orders = spark.read.table(\"LIVE.orders_union_bronze\") #use the new live table instead of the old one\n",
    "  return df_orders.join(df_customer, how=\"left_outer\", on=df_orders[\"o_custkey\"] == df_customer[\"c_custkey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f527077c-5cd2-4030-b2fa-e222a3f6d799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create another materialized view for silver layer version of the order_customer_view\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"This silver layer mat view will be used from the above view with additional column.\",\n",
    "  table_properties={\"quality\": \"silver\"},\n",
    "  name = \"customerOrders_silver\"\n",
    ")\n",
    "def order_customer_silver_view():\n",
    "  return spark.read.table(\"LIVE.order_customer_view\").withColumn(\"__insert_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ffe549-6fed-420e-9ec5-f3da334d0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create materialized view for gold layer that will have aggregations - order counts based on customer mkt segment\n",
    "from pyspark.sql.functions import current_timestamp,count,sum\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"This gold layer mat view will be used for aggregations.\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def orders_aggregated_gold():\n",
    "  df = spark.read.table(\"LIVE.customerOrders_silver\")\n",
    "  df_agg = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"orders_count_by_mkg_seg\"), sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ee4766-1b5c-48ae-922c-a7b5ecae7265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create dynamic tables for each order status type using the configuration settings\n",
    "for _status in order_status.split(\",\"):\n",
    "    @dlt.table(\n",
    "    comment=\"This gold layer mat view will be used for aggregations.\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\"\n",
    "    },\n",
    "    name = f\"orders_agg_{_status}_gold\"\n",
    "    )\n",
    "    def func():\n",
    "        df = spark.read.table(\"LIVE.customerOrders_silver\")\n",
    "        df_agg = df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"orders_count_by_mkg_seg\"), sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "        return df_agg "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}